---
title: "Análise de Resíduos em Modelos de Regressão Linear"
subtitle: "Testando Soluções com R"

output:
  bookdown::pdf_document2:
    latex_engine: xelatex
    fig_caption: yes
    fig_width: 3.5
    fig_height: 2.5
    keep_tex: no
    number_sections: no
    toc: no
    includes:
      in_header: header.tex

documentclass: article
geometry: margin=.5in
linestretch: 1.0
mainfont: Calibri
fontsize: 11pt
linkcolor: RoyalBlue
urlcolor: RoyalBlue
links-as-notes: false
---

```{r, echo=F, warning=F, message=F}
knitr::opts_chunk$set(echo = F,
                      warning = F,
                      message = F,
                      fig.align = "center")

# Função para padronizar o design dos gráficos
theme_graph <- function(){
  theme(
    plot.title = element_text(size = 10),
    plot.subtitle = element_text(size = 7),
    plot.caption = element_text(size = 6.5),
    axis.title = element_text(size = 6.5),
    axis.text = element_text(face = "italic", size = 6.5),
    legend.position = "right",
    legend.text = element_text(size = 6.5),
    legend.title = element_blank()
  )
}


table_design <- function(table,caption_set){
  table %>%
  kableExtra::kable_styling(position = "center",
                            font_size = 10,
                            full_width = F,
                            latex_options = c("HOLD_position",
                                              "stripped")) %>%
  kableExtra::row_spec(row = 0,
                       bold = T,
                       italic = F,
                       underline = F,
                       background = "black",
                       color = "white")
}
```

\linenumbers

O modelo de regressão linear é bastante usado na predição de variáveis contínuas, onde há uma ou mais variáveis independentes buscando mapear o comportamento de uma variável dependente. O modelo é bastante simples e lembro ser um dos primeiros a ser ensinado nas aulas de econometria. Porém, apesar de sua simplicidade, é preciso se atentar a alguns detalhes sobre suas premissas para que os resultados deste modelo possam ser usados para a tomada de decisão.

Abaixo vou listar algumas das premissas da regressão linear:

i) Ausência de multicolinearidade entre as variáveis independentes.

ii) Ausência de autocorrelação na variável dependente.

iii) Ausência de padrão no comportamento dos resíduos do modelo, ou seja, ausência de heterocedasticidade.

iv) Resíduos se distribuem de acordo com uma distribuição normal.

Tendo estas premissas atendidas, o modelo pode gerar conclusões confiáveis. Neste post eu vou desenvolver um modelo de regressão linear para testar as premissas **iii** e **iv** que tratam dos resíduos, para ver alguns casos práticos. O código usado neste post está nesta [pasta](https://github.com/FranciscoPiccolo/franciscopiccolo.github.io/blob/master/02.Posts_pdf/02.residual_analysis_in_econometric_models_20190905/article.Rmd) do meu Github.

```{r}
library(tidyverse)
library(kableExtra)
library(patchwork)
library(lmtest)
library(corrplot)
library(readxl)
```

# Exemplo prático: Elasticidade preço x oferta na produção de cana de açúcar

Este exemplo foi passado na minha aula de econometria em 2017. Na época o exercício foi realizado com o software **EViews**, por sorte eu guardei os dados e agora posso refazer o problema com mais facilidade com o uso do R. O dataset pode ser visualizado neste [link](https://github.com/FranciscoPiccolo/franciscopiccolo.github.io/blob/master/02.Posts_pdf/02.residual_analysis_in_econometric_models_20190905/datasets/dataset_1.csv).

No exercício, temos a variável independente (X) sendo o preço da cana de açúcar e a variável dependente (Y) sendo a área plantada de cana de açúcar (representando uma proxy para a oferta do produto). O objetivo deste modelo é tentar quantificar a elasticidade da oferta em função do preço, ou seja, quantificar quão sensível é a oferta da cana de açúcar quando ocorrem variações em seu preço.

```{r}
df <- read.csv(file = "https://raw.githubusercontent.com/FranciscoPiccolo/franciscopiccolo.github.io/master/01.residual_analysis_in_econometric_models_20190905/datasets/dataset_1.csv",
               sep = ";",
               dec = ",")
```

Abaixo há uma amostragem do dataset.

```{r}
df[sample(nrow(df),5), ] %>%
	kableExtra::kbl(caption = "Amostra do Dataset",
	                linesep = "\\addlinespace",
	                booktabs = T) %>%
    table_design()
```

O modelo de regressão linear para este cenário será desenvolvido de acordo com a fórmula abaixo:

$$ lnY_t = \beta_0+\beta_1 (lnX_t) + \mu_t $$

Onde:

$Y_t$ = Área plantada após a transformação com log natural (e)

$X_t$ = Preço da cana de açúcar também após a transformação com log natural

$\beta_0$ = Intercepto

$\beta_1$ = Inclinação

$\mu_t$ = Resíduos

Os dados precisam ter a aplicação do log natural, pois esta transformação faz com que as variações entre os períodos possam ser interpretadas como variações percentuais, e isso é necessário por conta de que a elasticidade é quantificada em termos percentuais. Esta característica ocorre apenas na transformação com logarítmo natural, se a transformação fosse feita com outros logs, a interpretação (de variaçõs percentuais) não seria válida.

O gráfico abaixo irá mostrar o comportamento das variáveis do dataset, bem como a curva de regressão linear, antes de aplicar a transformação log natural.

```{r}
df %>%
  ggplot()+
  geom_point(mapping = aes(x = price, y = area), shape = 1)+
  geom_smooth(mapping = aes(x = price, y = area),
              method = "lm",
              formula = y ~ x,
              se = F,
              lty = 2,
              color = "royal blue")+
  theme_graph()+
  labs(title = "Gráfico de dispersão das variáveis",
       x = "Preço da Cana de Açúcar",
       y = "Área Plantada")
```

Podemos ver que há uma relação entre o preço do produto e sua oferta (área plantada). Agora vamos aplicar o log natural no modelo de regressão. Para isso, o R nos fornece duas opções:

* Ajustar as variáveis no dataset e construir o modelo usando as variáveis ajustadas.

* Construir o modelo e indicar "dentro dele" que é necessário fazer a transformação antes de computar os resultados.

Vamos ver na prática como cada opção pode ser usada. O resultado final será idêntico.

Primeiro vou criar duas variáveis com os resultados dos dois métodos:

```{r}
# Método 1, criando novos campos no dataset com a transformação log (e)
first_method <-
  df %>%
  mutate(area_log = log(area),
         price_log = log(price)) %>%
  lm(formula = area_log ~ price_log)

second_method <-
  df %>%
  lm(formula = log(area) ~ log(price))
```

Com as duas variáveis criadas, vamos criar uma tabela comparando os principais resultados dos modelos:

```{r}
data.frame("1º Método" = c(first_method$coefficients),
           "2º Método" = c(second_method$coefficients)) %>%
  kableExtra::kbl(caption = "Comparativo dos Resultados",
                  linesep = "\\addlinespace",
                  booktabs = T) %>%
  table_design()
```

Conforme indicado, ambos os métodos geram o mesmo valor. Eu prefiro o segundo, que exige menos linhas de código. Com base nos coeficientes estimados, temos a seguinte equação:

$$\hat{Y} = 1.6416 + 0.9706X_1 + \mu$$
O resultado é estatisticamente significativo, visto que tanto o intercepto quanto a inclinação apresentam um valor-p baixo. Veja abaixo estes valores bem como o R².

```{r}
second_method %>%
  summary() %>%
  pander::pander()
```

Embora o modelo consiga explicar ~70% da variação na variável dependente, é preciso analisar os reíduos gerados para poder ter confiança no resultado e fazer projeções (objetivo principal). No gráfico abaixo, vamos ver como se distribui os resíduos do modelo em um Histograma e Gráfico QQ (quantile-quantile)

```{r}
model_residuals <-
  data.frame(values = second_method$residuals)
```

```{r, fig.width=5}
# Histogram
g1 <-
  model_residuals %>%
  ggplot()+
  geom_histogram(mapping = aes(x = values), fill = "steel blue")+
  theme_graph()+
  labs(title = "Distribuição dos \nResíduos",
       x = "",
       y = "")

# q-q plot
g2 <-
  model_residuals %>%
  ggplot()+
  geom_qq(mapping = aes(sample = values))+
  theme_graph()+
  labs(title = "Gráfico Q-Q",
       x = "",
       y = "")

g1 + g2
```

Ambos os gráficos indicam distribuição normal dos resíduos. Apesar desta forma visual ser recomendada, as vezes ela não é suficiente, casos em que o pesquisador precisará usar métodos mais formais para gerar uma conclusão. Para validação da independência no comportamento dos resíduos pode-se usar o teste **Durbin Watson**. O código abaixo realiza este teste.

```{r}
# Necessário instalar e chamar o pacote 'lmtest'
lmtest::dwtest(df %>%
                 lm(formula = log(area) ~ log(price))) %>%
  pander::pander()
```

O resultado do teste foi de 1.2912, mas apenas com este valor não é possível fazer uma conclusão. Em conjunto com este valor é preciso saber os valores limiares **DL** e **DU**, que podem ser encontrados nesta [tabela](http://www.portalaction.com.br/analise-de-regressao/33-diagnostico-de-independencia). Para encontrar os valores com nesta tabela, basta saber o número de observações no dataset (i.e. 33), o nível de significância do teste (i.e. 0.05) e os graus de liberdade (i.e. 1). Com isso, tem-se:

**DL** = 1.35

**DU** = 1.49

Tendo DW igual a 1.2912, acima de 0 e abaixo de DL, pode-se concluir que os resíduos são independentes.

Com isso, podemos concluir que de fato o modelo é confiável para realizar projeções, pois tanto os gráficos quanto o teste formal indicam que as premissas (iii) e (iv) estão sendo atendidas. Desta forma, podemos concluir que há elasticidade na oferta de cana de açúcar com relação ao seu preço.
